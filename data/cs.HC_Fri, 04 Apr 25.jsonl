{"main_page": "https://arxiv.org/abs/2504.02074", "pdf": "https://arxiv.org/pdf/2504.02074", "title": "Trapped by Expectations: Functional Fixedness in LLM-Enabled Chat Search", "authors": "Jiqun Liu, Jamshed Karimnazarov, Ryen W. White", "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)", "abstract": "Functional fixedness, a cognitive bias that restricts users' interactions with a new system or tool to expected or familiar ways, limits the full potential of Large Language Model (LLM)-enabled chat search, especially in complex and exploratory tasks. To investigate its impact, we conducted a crowdsourcing study with 450 participants, each completing one of six decision-making tasks spanning public safety, diet and health management, sustainability, and AI ethics. Participants engaged in a multi-prompt conversation with ChatGPT to address the task, allowing us to compare pre-chat intent-based expectations with observed interactions. We found that: 1) Several aspects of pre-chat expectations are closely associated with users' prior experiences with ChatGPT, search engines, and virtual assistants; 2) Prior system experience shapes language use and prompting behavior. Frequent ChatGPT users reduced deictic terms and hedge words and frequently adjusted prompts. Users with rich search experience maintained structured, less-conversational queries with minimal modifications. Users of virtual assistants favored directive, command-like prompts, reinforcing functional fixedness; 3) When the system failed to meet expectations, participants generated more detailed prompts with increased linguistic diversity, reflecting adaptive shifts. These findings suggest that while preconceived expectations constrain early interactions, unmet expectations can motivate behavioral adaptation. With appropriate system support, this may promote broader exploration of LLM capabilities. This work also introduces a typology for user intents in chat search and highlights the importance of mitigating functional fixedness to support more creative and analytical use of LLMs.", "content_excerpt": "Error extracting content: HTTP Error 404: Not Found"}
{"main_page": "https://arxiv.org/abs/2504.02110", "pdf": "https://arxiv.org/pdf/2504.02110", "title": "ScreenAudit: Detecting Screen Reader Accessibility Errors in Mobile Apps Using Large Language Models", "authors": "Mingyuan Zhong, Ruolin Chen, Xia Chen, James Fogarty, Jacob O. Wobbrock", "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)", "abstract": "Many mobile apps are inaccessible, thereby excluding people from their potential benefits. Existing rule-based accessibility checkers aim to mitigate these failures by identifying errors early during development but are constrained in the types of errors they can detect. We present ScreenAudit, an LLM-powered system designed to traverse mobile app screens, extract metadata and transcripts, and identify screen reader accessibility errors overlooked by existing checkers. We recruited six accessibility experts including one screen reader user to evaluate ScreenAudit's reports across 14 unique app screens. Our findings indicate that ScreenAudit achieves an average coverage of 69.2%, compared to only 31.3% with a widely-used accessibility checker. Expert feedback indicated that ScreenAudit delivered higher-quality feedback and addressed more aspects of screen reader accessibility compared to existing checkers, and that ScreenAudit would benefit app developers in real-world settings.", "content_excerpt": "\\setcctype\nby Mobile applications frequently fail to meet accessibility standards (Ross et\u00a0al., 2018; Yan and Ramachandran, 2019; Ross et\u00a0al., 2020; Fok et\u00a0al., 2022), rendering app contents and functionality inaccessible to people with disabilities.\nDespite efforts to promote developer awareness\u00a0(Apple, 2024b; Google, 2024c; W3C, 2021) and availability of developer toolkits\u00a0(Apple, 2024a; Google, 2024a; Deque, 2021),\na recent large-scale longitudinal study\u00a0(Fok et\u00a0al., 2022) found no significant improvements in mobile app accessibility.\nMeanwhile, an industry survey\u00a0(Access, 2024) found that 72% of developers are not certain that their company\u2019s digital services are accessible. Contributing to the lack of accessibility improvement and developer confidence may be the limited accessibility error coverage in current automated checkers (e.g., Google\u2019s Accessibility Scanner)\u00a0(Carvalho et\u00a0al., 2018; Mateus et\u00a0al., 2020; Google, 2024a).\nFor instance, Carvalho et al. found that blind and part..."}
{"main_page": "https://arxiv.org/abs/2504.02149", "pdf": "https://arxiv.org/pdf/2504.02149", "title": "Exploring the Privacy and Security Challenges Faced by Migrant Domestic Workers in Chinese Smart Homes", "authors": "Shijing He, Xiao Zhan, Yaxiong Lei, Yueyan Liu, Ruba Abu-Salma, Jose Such", "subjects": "Human-Computer Interaction (cs.HC); Cryptography and Security (cs.CR); Computers and Society (cs.CY)", "abstract": "The growing use of smart home devices poses considerable privacy and security challenges, especially for individuals like migrant domestic workers (MDWs) who may be surveilled by their employers. This paper explores the privacy and security challenges experienced by MDWs in multi-user smart homes through in-depth semi-structured interviews with 26 MDWs and 5 staff members of agencies that recruit and/or train domestic workers in China. Our findings reveal that the relationships between MDWs, their employers, and agencies are characterized by significant power imbalances, influenced by Chinese cultural and social factors (such as Confucianism and collectivism), as well as legal ones. Furthermore, the widespread and normalized use of surveillance technologies in China, particularly in public spaces, exacerbates these power imbalances, reinforcing a sense of constant monitoring and control. Drawing on our findings, we provide recommendations to domestic worker agencies and policymakers to address the privacy and security challenges facing MDWs in Chinese smart homes.", "content_excerpt": "\\setcctype\nby\n\n\n\n\n\n\n\n\n{CJK*}UTF8gbsn Homes around the world are getting smarter in recent years. The rapid adoption of smart home devices has significantly raised privacy and security concerns among various stakeholders, including device owners and primary users\u00a0(Zeng et\u00a0al., 2017; Zheng et\u00a0al., 2018; Abdi et\u00a0al., 2019; Huang et\u00a0al., 2020; Delgado\u00a0Rodriguez et\u00a0al., 2024; Le et\u00a0al., 2024), as well as bystanders\u00a0(Albayaydh and Flechais, 2024a; Despres et\u00a0al., 2024; Zhou et\u00a0al., 2024a; Park et\u00a0al., 2024; Marky et\u00a0al., 2024; Chiang et\u00a0al., 2024; Feger et\u00a0al., 2023; Shalawadi et\u00a0al., 2024; Wang et\u00a0al., 2023). In particular, recent literature has explored the privacy and security needs, concerns, and preferences of at-risk populations, including bystander groups who have little to no access to these devices and face challenges like power imbalances and privacy invasions in their living and/or work environments, as seen with migrant domestic workers (MDWs) who work in smart homes owned by the..."}
{"main_page": "https://arxiv.org/abs/2504.02176", "pdf": "https://arxiv.org/pdf/2504.02176", "title": "Unfiltered: How Teens Engage in Body Image and Shaming Discussions via Instagram Direct Messages (DMs)", "authors": "Abdulmalik Alluhidan, Jinkyung Katie Park, Mamtaj Akter, Rachel Rodgers, Afsaneh Razi, Pamela J. Wisniewski", "subjects": "Human-Computer Interaction (cs.HC)", "abstract": "We analyzed 1,596 sub-conversations within 451 direct message (DM) conversations from 67 teens (ages 13-17) who engaged in private discussions about body image on Instagram. Our findings show that teens often receive support when sharing struggles with negative body image, participate in criticism when engaging in body-shaming, and are met with appreciation when promoting positive body image. Additionally, these types of disclosures and responses varied based on whether the conversations were one-on-one or group-based. We found that sharing struggles and receiving support most often occurred in one-on-one conversations, while body shaming and negative interactions often occurred in group settings. A key insight of the study is that private social media settings can significantly influence how teens discuss and respond to body image. Based on these findings, we suggest design guidelines for social media platforms that could promote positive interactions around body image, ultimately creating a healthier and more supportive online environment for teens dealing with body image concerns.", "content_excerpt": "Body image is a critical aspect of adolescent development, significantly impacting mental health and self-esteem (Ajmal et\u00a0al., 2019; Choukas-Bradley et\u00a0al., 2022). In 2022, a nationally representative report in the United States showed that 73% of teenage girls and 69% of teenage boys had expressed significant self-consciousness regarding their physical appearance (Mostafavi, 2022).\nWith their visually centered design and social features, social media platforms such as Instagram have become central to how young people engage with one another about appearance\u00a0(Alluhidan et\u00a0al., 2024; DeVito et\u00a0al., 2017). Such public social media platforms have been reported to exacerbate body image concerns by showcasing idealized images, leading to increased body dissatisfaction through heightened social comparisons\u00a0(Pedalino and Camerini, 2022; Jung et\u00a0al., 2022).\nIn addition, within these environments, peer influences have been shown to play a crucial role in either exacerbating their body image co..."}
{"main_page": "https://arxiv.org/abs/2504.02204", "pdf": "https://arxiv.org/pdf/2504.02204", "title": "Characterizing Creativity in Visualization Design", "authors": "Naimul Hoque, Zinat Ara, Safwat Ali Khan, Fanny Chevalier, Niklas Elmqvist", "subjects": "Human-Computer Interaction (cs.HC)", "abstract": "Understanding the role of creativity in visualization design becomes increasingly important as the field matures, particularly with the emergence of various visualization authoring and recommendation systems. In this paper, we examine how creativity manifests in visualization design processes and how academic research has conceptualized it over time. Through a systematic review of 58 visualization papers that use the terms \"creativity\" or \"creative,\" we analyze the evolution of creative practices in visualization design. Our findings show that prior literature predominantly used atypical designs through free-form drawings, infographics, pictorials, and data comics to define creative representations. However, creativity in visualization design extends beyond visual representations to encompass early needfinding design activities such as sketching, storyboarding, discussion, and card sorting. Data visualization can also support a wide variety of creative tasks (e.g., fiction writing). We discuss the implications of these findings for fostering innovation within established design paradigms and for developing more sophisticated visualization authoring systems. The full list of coded papers are available here: this https URL.", "content_excerpt": "\\onlineid\n0\n\\vgtccategoryResearch\n\\vgtcpapertypetheory/model\n\n\n\\authorfooter\nNaimul Hoque is with University of Iowa, IA, USA.\nE-mail: nhoque@uiowa.edu.\nZinat Ara and Safwat Ali Khan are with George Mason University, VA, USA.\nE-mail: {zara, skhan89}@gmu.edu.\nFanny Chevalier is with University of Toronto, Toronto, ON, Canada.\nE-mail: fanny@dgp.toronto.edu.\nNiklas Elmqvist is with Aarhus University, Aarhus, Denmark.\nE-mail: elm@cs.au.dk. \\teaser\nDifferent forms of creativity in data visualization design. (Left) Design frameworks involving activities such as sketching, group discussion, and card sorting can foster human creativity regardless of the resulting visualization. (Middle) Infographics, pictorials, and data comics can promote creativity through unusual layouts and personalized glyphs and icons. These representations typically rely on author-driven creative activities such as designing custom glyphs and selecting image segments for pictorials. (Right) Data visualization can also s..."}
{"main_page": "https://arxiv.org/abs/2504.02217", "pdf": "https://arxiv.org/pdf/2504.02217", "title": "The Plot Thickens: Quantitative Part-by-Part Exploration of MLLM Visualization Literacy", "authors": "Matheus Valentim, Vaishali Dhanoa, Gabriela Molina Le\u00f3n, Niklas Elmqvist", "subjects": "Human-Computer Interaction (cs.HC)", "abstract": "Multimodal Large Language Models (MLLMs) can interpret data visualizations, but what makes a visualization understandable to these models? Do factors like color, shape, and text influence legibility, and how does this compare to human perception? In this paper, we build on prior work to systematically assess which visualization characteristics impact MLLM interpretability. We expanded the Visualization Literacy Assessment Test (VLAT) test set from 12 to 380 visualizations by varying plot types, colors, and titles. This allowed us to statistically analyze how these features affect model performance. Our findings suggest that while color palettes have no significant impact on accuracy, plot types and the type of title significantly affect MLLM performance. We observe similar trends for model omissions. Based on these insights, we look into which plot types are beneficial for MLLMs in different tasks and propose visualization design principles that enhance MLLM readability. Additionally, we make the extended VLAT test set, VLAT ex, publicly available on this https URL together with our supplemental material for future model testing and evaluation.", "content_excerpt": "\\onlineid\n0\n\\vgtccategoryResearch\n\\vgtcpapertypeTechnique\n\n\n\n\n\\authorfooter\nMatheus Valentim, Vaishali Dhanoa, Gabriela Molina Le\u00f3n, and Niklas Elmqvist are with Aarhus University in Aarhus, Denmark. E-mail: {au763015, dhanoa, leon, elm}@cs.au.dk.\n\n\\shortauthortitleThe Plot Thickens \\teaser\nVisualization literacy for MLLMs.\nOur work advances the field\u2019s knowledge about which charts and chart components play a role in the visualization literacy of multimodal large language models.\nThese findings suggest a set of common design principles that can help practitioners optimize data visualizations specifically for MLLMs. The rise of multimodal large language models (MLLMs) with their capacity to \u201csee\u201d\u2014or rather, translate images into vector embeddings\u2014opens up a world of possibility for the use of these models for visualization.\nThis new capability has prompted researchers to develop benchmarks that measure how well MLLMs interpret visualizations, drawing on established visualization literac..."}
{"main_page": "https://arxiv.org/abs/2504.02234", "pdf": "https://arxiv.org/pdf/2504.02234", "title": "LLM Social Simulations Are a Promising Research Method", "authors": "Jacy Reese Anthis, Ryan Liu, Sean M. Richardson, Austin C. Kozlowski, Bernard Koch, James Evans, Erik Brynjolfsson, Michael Bernstein", "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computers and Society (cs.CY)", "abstract": "Accurate and verifiable large language model (LLM) simulations of human research subjects promise an accessible data source for understanding human behavior and training new AI systems. However, results to date have been limited, and few social scientists have adopted these methods. In this position paper, we argue that the promise of LLM social simulations can be achieved by addressing five tractable challenges. We ground our argument in a literature survey of empirical comparisons between LLMs and human research subjects, commentaries on the topic, and related work. We identify promising directions with prompting, fine-tuning, and complementary methods. We believe that LLM social simulations can already be used for exploratory research, such as pilot experiments for psychology, economics, sociology, and marketing. More widespread use may soon be possible with rapidly advancing LLM capabilities, and researchers should prioritize developing conceptual models and evaluations that can be iteratively deployed and refined at pace with ongoing AI advances.", "content_excerpt": "\\WarningFilter\nlatexYou have requested package\n\\WarningFilterhyperrefIgnoring empty anchor\n\\WarningFiltercaptionUnknown document class  With the rapid advances in large language models (LLMs), many researchers have investigated their use for accurate and verifiable simulations of human research subjects (e.g., Hewitt et\u00a0al., 2024). Simulations promise to address many limitations of human data, such as difficulties of representative sampling (Henrich et\u00a0al., 2010), financial costs that limit accessibility (Alemayehu et\u00a0al., 2018), and methodological biases such as non-response bias (Sedgwick, 2014). Complementing human data with simulations could accelerate social science, open up new research opportunities\u2014such as exploring historical or cultural counterfactuals and piloting large-scale policy changes\u2014and fuel scalable synthetic data generation for the development of human-compatible AI (Bai et\u00a0al., 2022; Kim et\u00a0al., 2023). Nonetheless, the limitations of LLMs and simulation results to..."}
{"main_page": "https://arxiv.org/abs/2504.02250", "pdf": "https://arxiv.org/pdf/2504.02250", "title": "Designing Effective Human-Swarm Interaction Interfaces: Insights from a User Study on Task Performance", "authors": "Wasura D. Wattearachchi, Erandi Lakshika, Kathryn Kasmarik, Michael Barlow", "subjects": "Human-Computer Interaction (cs.HC); Robotics (cs.RO)", "abstract": "In this paper, we present a systematic method of design for human-swarm interaction interfaces, combining theoretical insights with empirical evaluation. We first derive ten design principles from existing literature, apply them to key information dimensions identified through goal-directed task analysis and developed a tablet-based interface for a target search task. We then conducted a user study with 31 participants where humans were required to guide a robotic swarm to a target in the presence of three types of hazards that pose a risk to the robots: Distributed, Moving, and Spreading. Performance was measured based on the proximity of the robots to the target and the number of deactivated robots at the end of the task. Results indicate that at least one robot was bought closer to the target in 98% of tasks, demonstrating the interface's success fulfilling the primary objective of the task. Additionally, in nearly 67% of tasks, more than 50% of the robots reached the target. Moreover, particularly better performance was noted in moving hazards. Additionally, the interface appeared to help minimize robot deactivation, as evidenced by nearly 94% of tasks where participants managed to keep more than 50% of the robots active, ensuring that most of the swarm remained operational. However, its effectiveness varied across hazards, with robot deactivation being lowest in distributed hazard scenarios, suggesting that the interface provided the most support in these conditions.", "content_excerpt": "Error extracting content: HTTP Error 404: Not Found"}
{"main_page": "https://arxiv.org/abs/2504.02526", "pdf": "https://arxiv.org/pdf/2504.02526", "title": "Improving User Experience with FAICO: Towards a Framework for AI Communication in Human-AI Co-Creativity", "authors": "Jeba Rezwana, Corey Ford", "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)", "abstract": "How AI communicates with humans is crucial for effective human-AI co-creation. However, many existing co-creative AI tools cannot communicate effectively, limiting their potential as collaborators. This paper introduces our initial design of a Framework for designing AI Communication (FAICO) for co-creative AI based on a systematic review of 107 full-length papers. FAICO presents key aspects of AI communication and their impacts on user experience to guide the design of effective AI communication. We then show actionable ways to translate our framework into two practical tools: design cards for designers and a configuration tool for users. The design cards enable designers to consider AI communication strategies that cater to a diverse range of users in co-creative contexts, while the configuration tool empowers users to customize AI communication based on their needs and creative workflows. This paper contributes new insights within the literature on human-AI co-creativity and Human-Computer Interaction, focusing on designing AI communication to enhance user experience.", "content_excerpt": "Human-AI co-creativity involves humans and AI collaborating in a creative process as partners to produce creative artifacts, ideas or performances (Davis, 2013). Co-creativity research suggests (Liapis et\u00a0al., 2014) that when creativity emerges from human-AI interaction, it can surpass contributors\u2019 original creativity and intentions as novel ideas arise in the process. With the emergence of popular Generative AI (GenAI) systems with co-creative abilities such as ChatGPT (ope, linea), DALL-E (ope, lineb) and Midjourney (mid, line), human-AI co-creativity is making its way into mainstream life. We suggest that the next frontier of co-creative AI needs good collaborative skills in addition to algorithmic competence. However, designing co-creative AI has many challenges due to the open-ended nature of the interaction between humans and AI in creative contexts (Davis et\u00a0al., 2016; Kantosalo et\u00a0al., 2014). For example, co-creative AI must be able to adapt to spontaneous interaction styles a..."}
{"main_page": "https://arxiv.org/abs/2504.02551", "pdf": "https://arxiv.org/pdf/2504.02551", "title": "Human-Centered Development of an Explainable AI Framework for Real-Time Surgical Risk Surveillance", "authors": "Andrea E Davidson, Jessica M Ray, Yulia Levites Strekalova, Parisa Rashidi, Azra Bihorac", "subjects": "Human-Computer Interaction (cs.HC)", "abstract": "Background: Artificial Intelligence (AI) clinical decision support (CDS) systems have the potential to augment surgical risk assessments, but successful adoption depends on an understanding of end-user needs and current workflows. This study reports the initial co-design of MySurgeryRisk, an AI CDS tool to predict the risk of nine post-operative complications in surgical patients. Methods: Semi-structured focus groups and interviews were held as co-design sessions with perioperative physicians at a tertiary academic hospital in the Southeastern United States. Participants were read a surgical vignette and asked questions to elicit an understanding of their current decision-making practices before being introduced to the MySurgeryRisk prototype web interface. They were asked to provide feedback on the user interface and system features. Session transcripts were qualitatively coded, after which thematic analysis took place. Results: Data saturation was reached after 20 surgeons and anesthesiologists from varying career stages participated across 11 co-design sessions. Thematic analysis resulted in five themes: (1) decision-making cognitive processes, (2) current approach to decision-making, (3) future approach to decision-making with MySurgeryRisk, (4) feedback on current MySurgeryRisk prototype, and (5) trustworthy considerations. Conclusion: Clinical providers perceived MySurgeryRisk as a promising CDS tool that factors in a large volume of data and is computed in real-time without any need for manual input. Participants provided feedback on the design of the interface and imaged applications of the tool in the clinical workflow. However, its successful implementation will depend on its actionability and explainability of model outputs, integration into current electronic systems, and calibration of trust among end-users.", "content_excerpt": "Error extracting content: HTTP Error 404: Not Found"}
{"main_page": "https://arxiv.org/abs/2504.02585", "pdf": "https://arxiv.org/pdf/2504.02585", "title": "\"I Feel Like I'm Teaching in a Gladiator Ring\": Barriers and Benefits of Live Coding in Classroom Settings", "authors": "Caroline Berger, David Weintrop, Niklas Elmqvist", "subjects": "Human-Computer Interaction (cs.HC)", "abstract": "Live coding for teaching-synchronously writing software in front of students-can be an effective method for engaging students and instilling practical programming skills. However, not all settings are conducive to live coding and not all instructors are successful in this challenging task. We present results from a study involving university instructors, teaching assistants, and students identifying both barriers and benefits of live coding. Physical infrastructure, a positive classroom community with psychological safety, and opportunities for teacher development are practical considerations for live coding. In order for live coding to be an active learning experience, we recommend that tools support multiple mechanisms for engaging students, directing audience attention, and encouraging student-led live coding.", "content_excerpt": "\u201cAnd then I\u2019m teaching [\u2026] in a classroom that feels like a gladiatorial ring.\n200 seats in a wall up in front of me.\nAnd I have to lean back to see the top.\nAnd really the only constraint in that classroom is that it\u2019s terrifying.\nIt is the most terrifying experience I\u2019ve ever had.\u201d\n\u2013 Participant 08 (Computer Science instructor) The gladiator descends into the arena engulfed by the shrill screams of the spectators.\nRows upon rows of the audience stares down at him, anticipating the long-awaited battle.\nBut we aren\u2019t in Ancient Rome; we are in a giant tiered lecture hall on a present-day university campus.\nThe spectators are students learning programming.\nAnd the gladiator?\nIt\u2019s the instructor, equipped with only their laptop and some notes, and the feat that they are about to attempt is to write valid source code in front of an auditorium full of students.\nWhile an effective means of engaging the audience and conveying authentic, practical programming skills, instructors liken this fo..."}
{"main_page": "https://arxiv.org/abs/2504.02622", "pdf": "https://arxiv.org/pdf/2504.02622", "title": "Exploring undercurrents of learning tensions in an LLM-enhanced landscape: A student-centered qualitative perspective on LLM vs Search", "authors": "Rahul R. Divekar, Sophia Guerra, Lisette Gonzalez, Natasha Boos, Helen Zhou", "subjects": "Human-Computer Interaction (cs.HC)", "abstract": "Large language models (LLMs) are transforming how students learn by providing readily available tools that can quickly augment or complete various learning activities with non-trivial performance. Similar paradigm shifts have occurred in the past with the introduction of search engines and Wikipedia, which replaced or supplemented traditional information sources such as libraries and books. This study investigates the potential for LLMs to represent the next shift in learning, focusing on their role in information discovery and synthesis compared to existing technologies, such as search engines. Using a within-subjects, counterbalanced design, participants learned new topics using a search engine (Google) and an LLM (ChatGPT). Post-task follow-up interviews explored students' reflections, preferences, pain points, and overall perceptions. We present analysis of their responses that show nuanced insights into when, why, and how students prefer LLMs over search engines, offering implications for educators, policymakers, and technology developers navigating the evolving educational landscape.", "content_excerpt": "Error extracting content: HTTP Error 404: Not Found"}
{"main_page": "https://arxiv.org/abs/2504.02624", "pdf": "https://arxiv.org/pdf/2504.02624", "title": "EmbodiedSense: Understanding Embodied Activities with Earphones", "authors": "Lixing He, Bufang Yang, Di Duan, Zhenyu Yan, Guoliang Xing", "subjects": "Human-Computer Interaction (cs.HC)", "abstract": "In this paper, we propose EmbodiedSense, a sensing system based on commercial earphones, which enables fine-grained activity logs using existing sensors. The activity logs record both user activities and the scenario in which the activities took place, benefiting detailed behavior understanding. By understanding both the user and the environment, EmbodiedSense addresses three main challenges: the limited recognition capability caused by information-hungry configurations (i.e., limited sensors available), the ineffective fusion to extract ambient information such as contextual scenarios, and the interference from ambient noise. Specifically, EmbodiedSense consists of a context-aware scenario recognition module and spatial-aware activity detection, which is further integrated with other attributes by expert knowledge. We implement our system on commercial earphones equipped with binaural microphones and an Inertial Measurement Unit (IMU). By distinguishing usage scenarios and identifying the source of sounds, EmbodiedSense enables fine-grained activity logs in a zero-shot manner (evaluated with up to 41 categories) and outperforms strong baselines like ImageBind-LLM by 38% F1-score. Extensive evaluations demonstrate that EmbodiedSense is a promising solution for long-term and short-term activity logs and provides significant benefits in monitoring the wearer's daily life.", "content_excerpt": "Embodied AI is a new and developing field of research that involves systems with a physical presence, allowing both robots and humans to interact with the real world (Bartolozzi et\u00a0al., 2022; Luo et\u00a0al., 2022).\nUnderstanding human behaviors is fundamental in embodied AI, which can be called embodied activity sensing. Embodied activity sensing is a significant extension of existing human activity recognition (HAR), which specifically focuses on user-conducted activity and is rarely explored.\nConsequently, embodied activity sensing can record a daily log that continuously tracks the users with both long-time scenarios and short-time activities, enabling fine-grained health monitoring (Munguia\u00a0Tapia, 2008; Alam and Roy, 2014; Gedam and Paul, 2021). For example, caregivers and doctors can refer to the detailed log to provide diagnoses and interventions for patients and the elderly. With the popularity of earphones, various applications\u00a0(Mollyn et\u00a0al., 2022; He et\u00a0al., 2023; Chen et\u00a0al., 20..."}
{"main_page": "https://arxiv.org/abs/2504.02663", "pdf": "https://arxiv.org/pdf/2504.02663", "title": "Development of Automated Data Quality Assessment and Evaluation Indices by Analytical Experience", "authors": "Yuka Haruki, Kei Kato, Yuki Enami, Hiroaki Takeuchi, Daiki Kazuno, Kotaro Yamada, Teruaki Hayashi", "subjects": "Human-Computer Interaction (cs.HC)", "abstract": "The societal need to leverage third-party data has driven the data-distribution market and increased the importance of data quality assessment (DQA) in data transactions between organizations. However, DQA requires expert knowledge of raw data and related data attributes, which hinders consensus-building in data purchasing. This study focused on the differences in DQAs between experienced and inexperienced data handlers. We performed two experiments: The first was a questionnaire survey involving 41 participants with varying levels of data-handling experience, who evaluated 12 data samples using 10 predefined indices with and without quality metadata generated by the automated tool. The second was an eye-tracking experiment to reveal the viewing behavior of participants during data evaluation. It was revealed that using quality metadata generated by the automated tool can reduce misrecognition in DQA. While experienced data handlers rated the quality metadata highly, semi-experienced users gave it the lowest ratings. This study contributes to enhancing data understanding within organizations and promoting the distribution of valuable data by proposing an automated tool to support DQAs.", "content_excerpt": "[1]\\fnmYuka \\surHaruki 1]\\orgdivSchool of Engineering, \\orgnameThe University of Tokyo\n2]\\orgdivKyodo Printing Co., Ltd. In recent years, data has increasingly become more of a commodity [1] and the volume of data distributed has grown with the importance of digital transformation [2][3]. Data has also been referred to as \u201cthe oil of the 21st century,\u201d and expectations of value creation using data have increased since the global boom in big data around 2013. In line with this, a data-distribution service emerged, where data is traded between companies and organizations [4]."}
{"main_page": "https://arxiv.org/abs/2504.02664", "pdf": "https://arxiv.org/pdf/2504.02664", "title": "How humans evaluate AI systems for person detection in automatic train operation: Not all misses are alike", "authors": "Romy M\u00fcller", "subjects": "Human-Computer Interaction (cs.HC)", "abstract": "If artificial intelligence (AI) is to be applied in safety-critical domains, its performance needs to be evaluated reliably. The present study aimed to understand how humans evaluate AI systems for person detection in automatic train operation. In three experiments, participants saw image sequences of people moving in the vicinity of railway tracks. A simulated AI had highlighted all detected people, sometimes correctly and sometimes not. Participants had to provide a numerical rating of the AI's performance and then verbally explain their rating. The experiments varied several factors that might influence human ratings: the types and plausibility of AI mistakes, the number of affected images, the number of people present in an image, the position of people relevant to the tracks, and the methods used to elicit human evaluations. While all these factors influenced human ratings, some effects were unexpected or deviated from normative standards. For instance, the factor with the strongest impact was people's position relative to the tracks, although participants had explicitly been instructed that the AI could not process such information. Taken together, the results suggest that humans may sometimes evaluate more than the AI's performance on the assigned task. Such mismatches between AI capabilities and human expectations should be taken into consideration when conducting safety audits of AI systems.", "content_excerpt": "Error extracting content: HTTP Error 404: Not Found"}
{"main_page": "https://arxiv.org/abs/2504.02675", "pdf": "https://arxiv.org/pdf/2504.02675", "title": "Cybersickness Assessment Framework(TestBed): Towards a Standardization of Experiments", "authors": "Nana Tian, Elif Kurtay, Dylan Vairoli, Adriano Viegas Milani, Ronan Boulic", "subjects": "Human-Computer Interaction (cs.HC)", "abstract": "Investigating cybersickness (CS) in virtual reality (VR) often requires significant resources to create the VR environment and manage other experiment-related aspects. Additionally, slight differences in VR content across studies can lead to conflicting results. To address these challenges, we propose a standardized assessment framework to facilitate cybersickness research. The main goal is to enable consistent and comparable CS-related experiments. By establishing this common foundation, researchers can better evaluate and compare the impact of various factors on cybersickness. We provide a comprehensive explanation of the conceptual designs, detail the technical implementation, and offer instructions for using the proposed framework. Lastly, we conclude by discussing the limitations and potential avenues for future development.", "content_excerpt": "Error extracting content: HTTP Error 404: Not Found"}
{"main_page": "https://arxiv.org/abs/2504.02735", "pdf": "https://arxiv.org/pdf/2504.02735", "title": "Pushing the Limit of PPG Sensing in Sedentary Conditions by Addressing Poor Skin-sensor Contact", "authors": "Manh Pham Hung, Matthew Yiwen Ho, Yiming Zhang, Dimitris Spathis, Aaqib Saeed, Dong Ma", "subjects": "Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)", "abstract": "Photoplethysmography (PPG) is a widely used non-invasive technique for monitoring cardiovascular health and various physiological parameters on consumer and medical devices. While motion artifacts are well-known challenges in dynamic settings, suboptimal skin-sensor contact in sedentary conditions - a critical issue often overlooked in existing literature - can distort PPG signal morphology, leading to the loss or shift of essential waveform features and therefore degrading sensing performance. In this work, we propose CP-PPG, a novel approach that transforms Contact Pressure-distorted PPG signals into ones with the ideal morphology. CP-PPG incorporates a novel data collection approach, a well-crafted signal processing pipeline, and an advanced deep adversarial model trained with a custom PPG-aware loss function. We validated CP-PPG through comprehensive evaluations, including 1) morphology transformation performance on our self-collected dataset, 2) downstream physiological monitoring performance on public datasets, and 3) in-the-wild performance. Extensive experiments demonstrate substantial and consistent improvements in signal fidelity (Mean Absolute Error: 0.09, 40% improvement over the original signal) as well as downstream performance across all evaluations in Heart Rate (HR), Heart Rate Variability (HRV), Respiration Rate (RR), and Blood Pressure (BP) estimation (on average, 21% improvement in HR; 41-46% in HRV; 6% in RR; and 4-5% in BP). These findings highlight the critical importance of addressing skin-sensor contact issues for accurate and dependable PPG-based physiological monitoring. Furthermore, CP-PPG can serve as a generic, plug-in API to enhance PPG signal quality.", "content_excerpt": "Photoplethysmography (PPG) is a non-invasive optical technique used to measure changes in blood volume in peripheral blood vessels. It functions by emitting light onto the skin using an LED and measuring the amount of light transmitted through (transmissive PPG) or reflected against (reflective PPG) the skin, which changes due to cardiac pulsations, with a photodetector. An ideal PPG waveform (Figure\u00a01) reflects complete cardiac cycles and contains all key features such as the systolic peak, dicrotic notch, and diastolic peak, all of which hold significant clinical importance. For example, PPG waveforms can be used to derive various physiological parameters, including heart rate (HR)\u00a0(Biswas et\u00a0al., 2019b; Thomas and Gopi, 2019; Arunkumar and Bhaskar, 2019), heart rate variability (HRV)\u00a0(Natarajan et\u00a0al., 2020; Taoum et\u00a0al., 2022; Hoog\u00a0Antink et\u00a0al., 2021; Aliani et\u00a0al., 2023), respiration rate (RR)\u00a0(Osathitporn et\u00a0al., 2023; Natarajan et\u00a0al., 2021), blood pressure (BP)\u00a0(Fan et\u00a0al., 20..."}
{"main_page": "https://arxiv.org/abs/2504.02780", "pdf": "https://arxiv.org/pdf/2504.02780", "title": "From Consumption to Collaboration: Measuring Interaction Patterns to Augment Human Cognition in Open-Ended Tasks", "authors": "Joshua Holstein, Moritz Diener, Philipp Spitzer", "subjects": "Human-Computer Interaction (cs.HC); Artificial Intelligence (cs.AI)", "abstract": "The rise of Generative AI, and Large Language Models (LLMs) in particular, is fundamentally changing cognitive processes in knowledge work, raising critical questions about their impact on human reasoning and problem-solving capabilities. As these AI systems become increasingly integrated into workflows, they offer unprecedented opportunities for augmenting human thinking while simultaneously risking cognitive erosion through passive consumption of generated answers. This tension is particularly pronounced in open-ended tasks, where effective solutions require deep contextualization and integration of domain knowledge. Unlike structured tasks with established metrics, measuring the quality of human-LLM interaction in such open-ended tasks poses significant challenges due to the absence of ground truth and the iterative nature of solution development. To address this, we present a framework that analyzes interaction patterns along two dimensions: cognitive activity mode (exploration vs. exploitation) and cognitive engagement mode (constructive vs. detrimental). This framework provides systematic measurements to evaluate when LLMs are effective tools for thought rather than substitutes for human cognition, advancing theoretical understanding and practical guidance for developing AI systems that protect and augment human cognitive capabilities.", "content_excerpt": "The emergence of Generative AI (GenAI) has fundamentally reshaped how humans approach cognitive tasks, from creative ideation to complex problem-solving (Radensky et\u00a0al., 2024; Tankelevitch et\u00a0al., 2024; Simkute et\u00a0al., 2025). Among these technologies, Large Language Models (LLMs) have become particularly influential due to their ability to engage in natural language dialogue and support a wide range of tasks (Subramonyam et\u00a0al., 2024). While LLMs show remarkable potential for augmenting human thinking and reflection (Simkute et\u00a0al., 2025), they simultaneously risk cognitive erosion through uncritical reliance on their outputs (Lee et\u00a0al., 2025; Sarkar et\u00a0al., 2024; Drosos et\u00a0al., 2025; Stadler et\u00a0al., 2024). This risk is particularly pronounced as LLMs can generate seemingly high-quality outputs that may contain substantial flaws (Russell et\u00a0al., 2024): they can produce hallucinations\u2014confidently stated but factually incorrect information\u2014that can lead to misinformation and flawed men..."}
{"main_page": "https://arxiv.org/abs/2504.02794", "pdf": "https://arxiv.org/pdf/2504.02794", "title": "MENA: Multimodal Epistemic Network Analysis for Visualizing Competencies and Emotions", "authors": "Behdokht Kiafar, Pavan Uttej Ravva, Asif Ahmmed Joy, Salam Daher, Roghayeh Leila Barmaki", "subjects": "Human-Computer Interaction (cs.HC)", "abstract": "The need to improve geriatric care quality presents a challenge that requires insights from stakeholders. While simulated trainings can boost competencies, extracting meaningful insights from these practices to enhance simulation effectiveness remains a challenge. In this study, we introduce Multimodal Epistemic Network Analysis (MENA), a novel framework for analyzing caregiver attitudes and emotions in an Augmented Reality setting and exploring how the awareness of a virtual geriatric patient (VGP) impacts these aspects. MENA enhances the capabilities of Epistemic Network Analysis by detecting positive emotions, enabling visualization and analysis of complex relationships between caregiving competencies and emotions in dynamic caregiving practices. The framework provides visual representations that demonstrate how participants provided more supportive care and engaged more effectively in person-centered caregiving with aware VGP. This method could be applicable in any setting that depends on dynamic interpersonal interactions, as it visualizes connections between key elements using network graphs and enables the direct comparison of multiple networks, thereby broadening its implications across various fields.", "content_excerpt": "\\onlineid\n1895\n\\vgtccategoryResearch\n\\vgtcpapertypetheory/model \\teaser\nThe architecture of the proposed multimodal Emotional State Classifier, consisting of four main components: Audio Extraction, Pose Estimation, Text Feature integrated with Knowledge Graph, and Fusion Network including a classification head. This setup determines whether the participant exhibited a positive emotion, such as a supportive and uplifting approach, during that segment of the video, as evidenced by their audio, gestures, and verbal communication. Introduction"}
